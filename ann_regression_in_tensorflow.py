# -*- coding: utf-8 -*-
"""ANN_regression_in_tensorflow.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BRrxRMqI0AdKB2-uOfXmdRPD3cS9OSVI

# Neural Network Regression with TensorFlow

## Creating data to view and fit
"""

import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf

# Create features (using tensors)
X = tf.constant([-7.0, -4.0, -1.0, 2.0, 5.0, 8.0, 11.0, 14.0])

# Create labels (using tensors)
y = tf.constant([3.0, 6.0, 9.0, 12.0, 15.0, 18.0, 21.0, 24.0]) # y= 1 * X + 10 (y = mx+c)

# Visualize it


plt.scatter(X, y);

# Set random seed
tf.random.set_seed(42)

# Create a model using the Sequential API
model = tf.keras.Sequential([
  tf.keras.layers.Dense(1),

])

 # Compile the model
model.compile(loss=tf.keras.losses.mae, # mae is short for mean absolute error
              optimizer=tf.keras.optimizers.SGD(), # SGD is short for stochastic gradient descent
              metrics=["mae"])
model.summary()

# Fit the model
model.fit(tf.expand_dims(X, axis=-1), y, epochs=5)

model.summary()

# Make a prediction with the model
result = model.predict(tf.expand_dims(np.array([17]), axis=-1))
print(result)

"""## Improving a model


"""

# Set random seed
tf.random.set_seed(42)

# Create a model (same as above)
model_1 = tf.keras.Sequential([
  tf.keras.layers.Dense(1),

])

# Compile model (same as above)
model_1.compile(loss=tf.keras.losses.mae,
              optimizer=tf.keras.optimizers.SGD(),
              metrics=["mae"])

# Fit model (this time we'll train for longer)
model_1.fit(tf.expand_dims(X, axis=-1), y, epochs=100) # train for 100 epochs not 10

# Try and predict what y would be if X was 17.0
result = model_1.predict(tf.expand_dims(np.array([17.0]), axis=-1))  # the right answer is 27.0 (y = X + 10)

print(result)

"""## Evaluating a model

"""

import numpy as np
# Make a bigger dataset
X = np.arange(-100, 100, 4)
X

# Make labels for the dataset (adhering to the same pattern as before)
y = np.arange(-90, 110, 4)
y

"""## Split data into training/test set

"""

# Create training and test sets
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state = 42) # set random state for reproducible splits

X_train

"""## Visualizing the data

"""

import matplotlib.pyplot as plt

plt.figure(figsize=(10, 7))
# Plot training data in blue
plt.scatter(X_train, y_train, c='b', label='Training data')
# Plot test data in green
plt.scatter(X_test, y_test, c='g', label='Testing data')
# Show the legend
plt.legend();

"""## Visualizing the model

>
"""

import tensorflow as tf
# Set random seed
tf.random.set_seed(42)

# Create a model (same as above)
model_2 = tf.keras.Sequential([
  tf.keras.layers.Dense(1, input_shape=[1]) # define the input_shape to our model
])

# Compile model (same as above)
model_2.compile(loss=tf.keras.losses.mae,
              optimizer=tf.keras.optimizers.SGD(),
              metrics=["mse"])

# This will work after specifying the input shape
model_2.summary()

# Fit the model to the training data
model_2.fit(X_train, y_train, epochs=100) # verbose controls how much gets output

# Check the model summary
model_2.summary()

"""Model summary using [`plot_model()`](https://www.tensorflow.org/api_docs/python/tf/keras/utils/plot_model)."""

# Try and predict what y would be if X was 17.0
a = model_2.predict(tf.expand_dims(np.array([17.0]), axis=-1))  # the right answer is 27.0 (y = X + 10)

print(a)

"""## Visualizing the predictions

"""

# Make predictions
y_preds = model_2.predict(X_test)

# View the predictions
y_preds

def plot_predictions(train_data=X_train,
                     train_labels=y_train,
                     test_data=X_test,
                     test_labels=y_test,
                     predictions=y_preds):
  """
  Plots training data, test data and compares predictions.
  """
  plt.figure(figsize=(10, 7))
  # Plot training data in blue
  plt.scatter(train_data, train_labels, c="b", label="Training data")
  # Plot test data in green
  plt.scatter(test_data, test_labels, c="g", label="Testing data")
  # Plot the predictions in red (predictions were made on the test data)
  plt.scatter(test_data, predictions, c="r", label="Predictions")
  # Show the legend
  plt.legend();

plot_predictions(train_data=X_train,
                 train_labels=y_train,
                 test_data=X_test,
                 test_labels=y_test,
                 predictions=y_preds)

"""## Evaluating predictions

"""

# Evaluate the model on the test set
model_2.evaluate(X_test, y_test)

"""## Running experiments to improve a model


**Build `model_1`**
"""

# Set random seed
tf.random.set_seed(42)

# Replicate original model
model_3 = tf.keras.Sequential([
  tf.keras.layers.Dense(1)
])

# Compile the model
model_3.compile(loss=tf.keras.losses.mae,
                optimizer=tf.keras.optimizers.SGD(),
                metrics=['mae'])

# Fit the model
model_3.fit(tf.expand_dims(X_train, axis=-1), y_train, epochs=500)

# Make and plot predictions for model_1
y_preds_1 = model_3.predict(X_test)
plot_predictions(predictions=y_preds_1)

# Try and predict what y would be if X was 17.0
a = model_3.predict(tf.expand_dims(np.array([17.0]), axis=-1))  # the right answer is 27.0 (y = X + 10)

print(a)

mae_1 = tf.keras.losses.MAE(y_test, y_preds_1.squeeze()).numpy()
mse_1 = tf.keras.losses.MSE(y_test, y_preds_1.squeeze()).numpy()
mae_1, mse_1

"""## Saving a model"""

# Save a model
model_3.save("best_model.keras")

"""## Loading a model

"""

# Load a model from the SavedModel format
loaded_saved_model = tf.keras.models.load_model('best_model.keras')
loaded_saved_model.summary()

y_preds_lsm = loaded_saved_model.predict(X_test)

mae_lsm = tf.keras.losses.MAE(y_test, y_preds_lsm.squeeze()).numpy()
mse_lsm = tf.keras.losses.MSE(y_test, y_preds_lsm.squeeze()).numpy()
mae_lsm, mse_lsm

# Set random seed
tf.random.set_seed(42)

# Replicate original model
model_4 = tf.keras.Sequential([
  tf.keras.layers.Dense(1)
])

# Compile the model
model_4.compile(loss=tf.keras.losses.mae,
                optimizer=tf.keras.optimizers.Adam(),
                metrics=['mae'])

# Fit the model
model_4.fit(tf.expand_dims(X_train, axis=-1), y_train, epochs=100,verbose = 0)

# Make and plot predictions for model_1
y_preds_2 = model_4.predict(X_test)
plot_predictions(predictions=y_preds_2)

# Set random seed
tf.random.set_seed(42)

# Replicate original model
model_5 = tf.keras.Sequential([
  tf.keras.layers.Dense(10),
  tf.keras.layers.Dense(1)
])

# Compile the model
model_5.compile(loss=tf.keras.losses.mae,
                optimizer=tf.keras.optimizers.SGD(),
                metrics=['mae'])

# Fit the model
model_5.fit(tf.expand_dims(X_train, axis=-1), y_train, epochs=100,verbose = 0)

# Make and plot predictions for model_1
y_preds_3 = model_5.predict(X_test)
plot_predictions(predictions=y_preds_3)

mae_lsm = tf.keras.losses.MAE(y_test, y_preds_3.squeeze()).numpy()
mse_lsm = tf.keras.losses.MSE(y_test, y_preds_3.squeeze()).numpy()
mae_lsm, mse_lsm

"""# Insurance dataset"""

# Import required libraries
import tensorflow as tf
import pandas as pd
import matplotlib.pyplot as plt

# Read in the insurance dataset
insurance = pd.read_csv("https://raw.githubusercontent.com/stedy/Machine-Learning-with-R-datasets/master/insurance.csv")

# Check out the insurance dataset
insurance.head()

insurance.tail()

insurance.info()

insurance.describe()

# Turn all categories into numbers
insurance_one_hot = pd.get_dummies(insurance)
insurance_one_hot.head() # view the converted columns

insurance_one_hot.info()

"""Now we'll split data into features (`X`) and labels (`y`)."""

# Create X & y values
X = insurance_one_hot.drop("charges", axis=1)
y = insurance_one_hot["charges"]

# View features
X.head()

y.head()

# Create training and test sets
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state = 42) # set random state for reproducible splits

"""# Model 1 with optimise SGD()"""

# Set random seed
tf.random.set_seed(42)

# Create a new model (same as model_2)
insurance_model = tf.keras.Sequential([
  tf.keras.layers.Dense(10),
  tf.keras.layers.Dense(1)
])

# Compile the model
insurance_model.compile(loss=tf.keras.losses.mae,
                          optimizer=tf.keras.optimizers.SGD(),
                          metrics=['mae'])

# Fit the model and save the history (we can plot this)
history = insurance_model.fit(X_train, y_train, epochs=100)

insurance_model.summary()

insurance_model_loss, insurance_model_mae = insurance_model.evaluate(X_test, y_test)
insurance_model_loss, insurance_model_mae

# Plot history (also known as a loss curve)
pd.DataFrame(history.history).plot()
plt.ylabel("loss")
plt.xlabel("epochs");

"""# Model 2 with 100 epochs and Adam"""

# Set random seed
tf.random.set_seed(42)

# Build the model (3 layers, 100, 10, 1 units)
insurance_model_1 = tf.keras.Sequential([
  tf.keras.layers.Dense(100),
  tf.keras.layers.Dense(10),
  tf.keras.layers.Dense(1)
])

# Compile the model
insurance_model_1.compile(loss=tf.keras.losses.mae,
                          optimizer=tf.keras.optimizers.Adam(),
                          metrics=['mae'])

# Fit the model for 200 epochs (same as insurance_model_2)
history1 = insurance_model_1.fit(X_train, y_train, epochs=100, verbose=0)

insurance_model_1.summary()

insurance_model_1_loss, insurance_model_1_mae = insurance_model_1.evaluate(X_test, y_test)
insurance_model_1_loss, insurance_model_1_mae

# Plot history (also known as a loss curve)
pd.DataFrame(history1.history).plot()
plt.ylabel("loss")
plt.xlabel("epochs");

"""# Model 3 with 200 epochs and Adam"""

# Set random seed
tf.random.set_seed(42)

# Build the model (3 layers, 100, 10, 1 units)
insurance_model_2 = tf.keras.Sequential([
  tf.keras.layers.Dense(100),
  tf.keras.layers.Dense(10),
  tf.keras.layers.Dense(1)
])

# Compile the model
insurance_model_2.compile(loss=tf.keras.losses.mae,
                          optimizer=tf.keras.optimizers.Adam(),
                          metrics=['mae'])

# Fit the model for 200 epochs (same as insurance_model_2)
history2 = insurance_model_2.fit(X_train, y_train, epochs=200, verbose=0)

# Evaluate the model trained for 200 total epochs
insurance_model_2_loss, insurance_model_2_mae = insurance_model_2.evaluate(X_test, y_test)
insurance_model_2_loss, insurance_model_2_mae

# Plot the model trained for 200 total epochs loss curves
pd.DataFrame(history2.history).plot()
plt.ylabel("loss")
plt.xlabel("epochs"); # note: epochs will only show 100 since we overrid the history variable

"""## Preprocessing data (normalization and standardization)"""

import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf

# Read in the insurance dataset
insurance = pd.read_csv("https://raw.githubusercontent.com/stedy/Machine-Learning-with-R-datasets/master/insurance.csv")

# Check out the data
insurance.head()

from sklearn.compose import make_column_transformer
from sklearn.preprocessing import MinMaxScaler, OneHotEncoder
from sklearn.model_selection import train_test_split

# Create column transformer (this will help us normalize/preprocess our data)
ct = make_column_transformer(
    (MinMaxScaler(), ["age", "bmi", "children"]), # get all values between 0 and 1
    (OneHotEncoder(handle_unknown="ignore"), ["sex", "smoker", "region"])
)

# Create X & y
X = insurance.drop("charges", axis=1)
y = insurance["charges"]

# Build our train and test sets (use random state to ensure same split as before)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Fit column transformer on the training data only (doing so on test data would result in data leakage)
ct.fit(X_train)

# Transform training and test data with normalization (MinMaxScalar) and one hot encoding (OneHotEncoder)
X_train_normal = ct.transform(X_train)
X_test_normal = ct.transform(X_test)

# Non-normalized and non-one-hot encoded data example
X_train.loc[0]

# Normalized and one-hot encoded example
X_train_normal[0]

# Notice the normalized/one-hot encoded shape is larger because of the extra columns
X_train_normal.shape, X_train.shape

"""

 `insurance_model_2`."""

# Set random seed
tf.random.set_seed(42)

# Build the model (3 layers, 100, 10, 1 units)
insurance_model_3 = tf.keras.Sequential([
  tf.keras.layers.Dense(100),
  tf.keras.layers.Dense(10),
  tf.keras.layers.Dense(1)
])

# Compile the model
insurance_model_3.compile(loss=tf.keras.losses.mae,
                          optimizer=tf.keras.optimizers.Adam(),
                          metrics=['mae'])

# Fit the model for 200 epochs (same as insurance_model_2)
history3 = insurance_model_3.fit(X_train_normal, y_train, epochs=200, verbose=0)

# Evaulate 3rd model
insurance_model_3_loss, insurance_model_3_mae = insurance_model_3.evaluate(X_test_normal, y_test)

# Plot history (also known as a loss curve)
pd.DataFrame(history3.history).plot()
plt.ylabel("loss")
plt.xlabel("epochs");

# Compare modelling results from non-normalized data and normalized data
model_comapare = pd.DataFrame(data=[insurance_model_mae, insurance_model_1_mae, insurance_model_2_mae, insurance_model_3_mae],
                              # Pass data as a list
                              columns=['Model MAE'])  # Specify column name
model_comapare

#VISUALIZE THE PERFORMANCE
import matplotlib.pyplot as plt
import numpy as np

# Create a list of model names and their corresponding MAE values
models = ['Model 1', 'Model 2', 'Model 3','Model 4']
mae_values = [insurance_model_mae,insurance_model_1_mae, insurance_model_2_mae, insurance_model_3_mae]

# Create the bar plot
plt.bar(models, mae_values,width = 0.3)

# Add labels and title
plt.xlabel('Insurance Models')
plt.ylabel('Mean Absolute Error (MAE)')
plt.title('Comparison of Insurance Model Performance')

for i, v in enumerate(mae_values):
    plt.text(i, v + 50, str(round(v, 2)), ha='center', va='bottom')

# Display the plot
plt.show()